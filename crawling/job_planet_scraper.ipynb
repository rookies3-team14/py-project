{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium webdriver-manager beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1291264, 'annual_text': '경력'}, {'id': 1287688, 'annual_text': '경력'}, {'id': 1287613, 'annual_text': '경력'}, {'id': 1290553, 'annual_text': '경력'}, {'id': 1288726, 'annual_text': '경력'}, {'id': 1290918, 'annual_text': '경력'}, {'id': 1283031, 'annual_text': '경력'}, {'id': 1291090, 'annual_text': '경력'}, {'id': 1291054, 'annual_text': '경력'}, {'id': 1291048, 'annual_text': '경력'}, {'id': 1291100, 'annual_text': '경력'}, {'id': 1291099, 'annual_text': '경력'}, {'id': 1291053, 'annual_text': '경력'}, {'id': 1287485, 'annual_text': '경력'}, {'id': 1287482, 'annual_text': '경력'}, {'id': 1290554, 'annual_text': '경력'}]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "\n",
    "class JobPlanetCrawler:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()))\n",
    "        self.driver.implicitly_wait(3)\n",
    "\n",
    "    def get_html(self):\n",
    "        # HTML 소스 가져오기\n",
    "        self.driver.get(self.url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def parse_json(self, html):\n",
    "        # HTML에서 JSON 데이터를 파싱하여 반환\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        json_text = soup.find(\"pre\").text if soup.find(\"pre\") else \"\"\n",
    "        if json_text:\n",
    "            return json.loads(json_text)\n",
    "        else:\n",
    "            print(\"❌ <pre> 태그를 찾을 수 없습니다.\")\n",
    "            return None\n",
    "\n",
    "    def extract_job_data(self, data):\n",
    "        # JSON 데이터에서 id,경력 값 추출\n",
    "        jobs = [\n",
    "            {\n",
    "                \"id\": job.get(\"id\"),\n",
    "                \"annual_text\": job.get(\"annual\", {}).get(\"text\")\n",
    "            }\n",
    "            for job in data.get(\"data\", {}).get(\"recruits\", [])\n",
    "        ]\n",
    "        return jobs\n",
    "\n",
    "    def crawl_jobs(self):\n",
    "        # 크롤링 실행 후 결과를 반환\n",
    "        html = self.get_html()\n",
    "        data = self.parse_json(html)\n",
    "        if data:\n",
    "            jobs = self.extract_job_data(data)\n",
    "            return jobs\n",
    "        return []\n",
    "\n",
    "    def quit(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "job_data = []\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(1, 36):\n",
    "        url = f\"https://www.jobplanet.co.kr/api/v3/job/postings?occupation_level1=&occupation_level2=11905,11907,11904,11906,11610,11911,11609&years_of_experience=&review_score=&job_type=&city=&education_level_id=&order_by=aggressive&page={i}&page_size=8\"\n",
    "        crawler = JobPlanetCrawler(url)\n",
    "        job_data.extend(crawler.crawl_jobs())\n",
    "\n",
    "    # if job_data:\n",
    "    #     print(json.dumps(job_data, ensure_ascii=False, indent=2))\n",
    "\n",
    "    crawler.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobplanet_recruitment_text(obj: dict) -> list:\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.implicitly_wait(50)\n",
    "    url = f\"https://www.jobplanet.co.kr/job/search?posting_ids%5B%5D={obj[\"id\"]}\"\n",
    "\n",
    "    print(url)\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    elements = soup.select(\"p.recruitment-detail__txt\")\n",
    "\n",
    "    if elements:\n",
    "        flat_page_texts = [\n",
    "            word.lower()\n",
    "            for elem in elements\n",
    "            for word in re.findall(r'\\b[a-zA-Z0-9]+\\b', elem.text)\n",
    "        ]\n",
    "        driver.quit()\n",
    "        return flat_page_texts\n",
    "    else:\n",
    "        print(\"텍스트가 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import job_list\n",
    "\n",
    "front_list = job_list.FRONT_STACK_LIST\n",
    "back_list =job_list.BACK_STACK_LIST\n",
    "ios_list = job_list.IOS_STACK_LIST\n",
    "cross_list = job_list.CROSS_STACK_LIST\n",
    "android_list = job_list.ANDROID_STACK_LIST\n",
    "game_list =job_list.GAME_STACK_LIST\n",
    "security_list = job_list.SECURITY_STACK_LIST\n",
    "cloud_list = job_list.CLOUD_STACK_LIST\n",
    "\n",
    "entire_list = [front_list, back_list, ios_list, cross_list, android_list, game_list, security_list, cloud_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_data(data_list, elist, count_total):\n",
    "    count_s = {}\n",
    "    for d in data_list:\n",
    "        for e in elist:  \n",
    "            if d.strip() == e.strip():\n",
    "                if d not in count_s:\n",
    "                    count_s[d] = 1\n",
    "    count_total.append(count_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path  # 파일 존재 여부를 체크하기 위한 모듈\n",
    "\n",
    "def write_excel(data_list):\n",
    "    count_total = list()\n",
    "    for e in entire_list:\n",
    "        count_data(data_list, e, count_total)\n",
    "\n",
    "    print(count_total)\n",
    "    \n",
    "    file_name = '../data/excel/StackList.xlsx'\n",
    "    sheet_names = ['FRONT_STACK_LIST','BACK_STACK_LIST','IOS_STACK_LIST', 'CROSS_STACK_LIST', 'ANDROID_STACK_LIST', 'GAME_STACK_LIST', \n",
    "                'SECURITY_STACK_LIST', 'CLOUD_STACK_LIST']\n",
    "    df_list = list()\n",
    "    for i in range(len(sheet_names)):\n",
    "        df = pd.DataFrame(list(count_total[i].items()), columns= ['stack', 'count'])\n",
    "        df.set_index('stack', inplace=True)  # stack을 인덱스로 설정\n",
    "        df_list.append(df)\n",
    "\n",
    "    total_df_list = list()\n",
    "    if Path(file_name).exists():\n",
    "        try:\n",
    "            for i in range(len(sheet_names)):\n",
    "                old_df = pd.read_excel(file_name, sheet_name=sheet_names[i], index_col=0)\n",
    "                total_df_list.append(old_df.add(df_list[i], fill_value=0))\n",
    "        except:\n",
    "            # 시트가 엑셀 파일 안에 없을 때\n",
    "            total_df_list = df_list\n",
    "    #엑셀 파일 없을 때\n",
    "    else:\n",
    "        total_df_list = df_list\n",
    "        \n",
    "    if Path(file_name).exists():\n",
    "        with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "            for sheet, df in zip(sheet_names, total_df_list):\n",
    "                df.to_excel(writer, sheet_name=sheet)\n",
    "    else:\n",
    "        with pd.ExcelWriter(file_name, engine='openpyxl', mode='w') as writer:\n",
    "            for sheet, df in zip(sheet_names, total_df_list):\n",
    "                df.to_excel(writer, sheet_name=sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jobplanet.co.kr/job/search?posting_ids%5B%5D=1291264\n",
      "[{}, {'python': 1, 'docker': 1, 'flask': 1, 'redis': 1, 'kubernetes': 1}, {}, {}, {}, {}, {}, {'docker': 1, 'kubernetes': 1, 'gcp': 1, 'aws': 1}]\n",
      "https://www.jobplanet.co.kr/job/search?posting_ids%5B%5D=1287688\n",
      "[{}, {'python': 1, 'django': 1, 'mysql': 1, 'dynamodb': 1, 'kubernetes': 1, 'redis': 1}, {}, {}, {}, {}, {'prometheus': 1}, {'kubernetes': 1, 'spinnaker': 1, 'datadog': 1, 'grafana': 1, 'prometheus': 1, 'loki': 1, 'sentry': 1, 'aws': 1}]\n"
     ]
    }
   ],
   "source": [
    "for d in job_data:\n",
    "    text=get_jobplanet_recruitment_text(d)\n",
    "    write_excel(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
